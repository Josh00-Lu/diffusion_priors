{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FFHQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from unet import UNetModel\n",
    "from attr_classifier import FaceAttrModel\n",
    "from diffusion import GaussianDiffusion\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76d526-8b35-4943-931c-a0f4249dd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DDPM model\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "diff_net = UNetModel(image_size=256, in_channels=3, out_channels=6, \n",
    "                     model_channels=256, num_res_blocks=2, channel_mult=(1, 1, 2, 2, 4, 4),\n",
    "                     attention_resolutions=[32,16,8], num_head_channels=64, dropout=0.1, resblock_updown=True, use_scale_shift_norm=True).to(device)\n",
    "diff_net.load_state_dict(torch.load('models/ffhq.pt'))\n",
    "print('Loaded Diffusion Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a50ef-99c7-4515-bf8a-b53c42e7d9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X0 = torch.randn(1, 3, 256, 256).to(device) #初始化一个白噪声X0\n",
    "X0.requires_grad_(True)\n",
    "# Inference procedure steps\n",
    "steps = 250   \n",
    "\n",
    "opt = torch.optim.Adamax([X0], lr=1)\n",
    "\n",
    "diffusion = GaussianDiffusion(T=1000, schedule='linear')\n",
    "diff_net.eval()\n",
    "\n",
    "bar = tqdm.tqdm(range(steps))\n",
    "for i, _ in enumerate(bar):\n",
    "    # Select t      \n",
    "    t = ((steps-i)/1.5 + (steps-i)/3*math.cos(i/10))/steps*800 + 200 # Linearly decreasing + cosine\n",
    "    t = np.array([t + np.random.randint(-50, 51) for _ in range(1)]).astype(int) # Add noise to t\n",
    "    t = np.clip(t, 1, diffusion.T)\n",
    "       \n",
    "    # Denoise\n",
    "    sample_img = X0\n",
    "    xt, epsilon = diffusion.sample(sample_img, t)       \n",
    "    t = torch.from_numpy(t).float().view(1)    \n",
    "    pred = diff_net(xt.float(), t.to(device))   \n",
    "    epsilon_pred = pred[:,:3,:,:] # Use predicted noise only\n",
    "    \n",
    "    loss = F.mse_loss(epsilon_pred, epsilon)\n",
    "    bar.set_description(\"Loss: {:.4f}\".format(loss))\n",
    "\n",
    "    opt.zero_grad()        \n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        grad_norm = torch.linalg.norm(X0)\n",
    "        if i > 0:\n",
    "            alpha = 0.5\n",
    "            norm_track = alpha*norm_track + (1-alpha)*grad_norm\n",
    "        else:\n",
    "            norm_track = grad_norm \n",
    "                  \n",
    "    opt.step()\n",
    "\n",
    "    if (i+1) % 25 == 0 or i == 0:\n",
    "        with torch.no_grad():\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "            ax.imshow(0.5*(X0+1)[0].cpu().numpy().transpose([1,2,0]))\n",
    "            ax.set_title('Inferred Image')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682887c",
   "metadata": {},
   "source": [
    "## 2. ImagNet uncond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c34c4d-1563-47e1-aaf0-b33635d7e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from unet import UNetModel, EncoderUNetModel\n",
    "from attr_classifier import FaceAttrModel\n",
    "from diffusion import GaussianDiffusion\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbce21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DDPM model\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "diff_net = UNetModel(image_size=256, in_channels=3, out_channels=6,\n",
    "                     model_channels=256, num_res_blocks=2, channel_mult=(1, 1, 2, 2, 4, 4),\n",
    "                     attention_resolutions=[32,16,8], num_head_channels=64, dropout=0.0, resblock_updown=True, use_scale_shift_norm=True).to(device)\n",
    "diff_net.load_state_dict(torch.load('/root/Desktop/diffusionclean/guided-diffusion/models/256x256_diffusion_uncond.pt'))\n",
    "print('Loaded Diffusion Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a79033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(\n",
    "    image_size,\n",
    "    classifier_use_fp16,\n",
    "    classifier_width,\n",
    "    classifier_depth,\n",
    "    classifier_attention_resolutions,\n",
    "    classifier_use_scale_shift_norm,\n",
    "    classifier_resblock_updown,\n",
    "    classifier_pool,\n",
    "):\n",
    "    if image_size == 512:\n",
    "        channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n",
    "    elif image_size == 256:\n",
    "        channel_mult = (1, 1, 2, 2, 4, 4)\n",
    "    elif image_size == 128:\n",
    "        channel_mult = (1, 1, 2, 3, 4)\n",
    "    elif image_size == 64:\n",
    "        channel_mult = (1, 2, 3, 4)\n",
    "    else:\n",
    "        raise ValueError(f\"unsupported image size: {image_size}\")\n",
    "\n",
    "    attention_ds = []\n",
    "    for res in classifier_attention_resolutions.split(\",\"):\n",
    "        attention_ds.append(image_size // int(res))\n",
    "\n",
    "    return EncoderUNetModel(\n",
    "        image_size=image_size,\n",
    "        in_channels=3,\n",
    "        model_channels=classifier_width,\n",
    "        out_channels=1000,\n",
    "        num_res_blocks=classifier_depth,\n",
    "        attention_resolutions=tuple(attention_ds),\n",
    "        channel_mult=channel_mult,\n",
    "        use_fp16=classifier_use_fp16,\n",
    "        num_head_channels=64,\n",
    "        use_scale_shift_norm=classifier_use_scale_shift_norm,\n",
    "        resblock_updown=classifier_resblock_updown,\n",
    "        pool=classifier_pool,\n",
    "    )\n",
    "\n",
    "classifier = create_classifier(\n",
    "    image_size=256, \n",
    "    classifier_use_fp16=False, classifier_width=128, classifier_depth=2,\n",
    "    classifier_attention_resolutions=\"32,16,8\",  # 16\n",
    "    classifier_use_scale_shift_norm=True,  # False\n",
    "    classifier_resblock_updown=True,  # False\n",
    "    classifier_pool=\"attention\"\n",
    ").to(device)\n",
    "\n",
    "classifier.load_state_dict(torch.load(\"/root/Desktop/diffusionclean/guided-diffusion/models/256x256_clean_classifier.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a50ef-99c7-4515-bf8a-b53c42e7d9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InferenceModel, self).__init__()\n",
    "        # Inferred image\n",
    "        self.img = nn.Parameter(torch.randn(1,3,256,256))\n",
    "        self.img.requires_grad = True\n",
    "\n",
    "    def encode(self):\n",
    "        return self.img\n",
    "\n",
    "X0 = InferenceModel().to(device)\n",
    "# Inference procedure steps\n",
    "steps = 250   \n",
    "\n",
    "opt = torch.optim.Adamax(X0.parameters(), lr=1)\n",
    "\n",
    "diffusion = GaussianDiffusion(T=1000, schedule='linear')\n",
    "diff_net.eval()\n",
    "\n",
    "classes = torch.randint(low=0, high=1000, size=(1,), device=device)\n",
    "\n",
    "bar = tqdm.tqdm(range(steps))\n",
    "for i, _ in enumerate(bar):\n",
    "    # Select t      \n",
    "    t = ((steps-i)/1.5 + (steps-i)/3*math.cos(i/10))/steps*800 + 200 # Linearly decreasing + cosine\n",
    "    t = np.array([t + np.random.randint(-50, 51) for _ in range(1)]).astype(int) # Add noise to t\n",
    "    t = np.clip(t, 1, diffusion.T)\n",
    "       \n",
    "    # Denoise\n",
    "    sample_img = X0.encode()\n",
    "    xt, epsilon = diffusion.sample(sample_img, t)       \n",
    "    t = torch.from_numpy(t).float().view(1)    \n",
    "    pred = diff_net(xt.float(), t.to(device))   \n",
    "    epsilon_pred = pred[:,:3,:,:] # Use predicted noise only\n",
    "    \n",
    "    loss = F.mse_loss(epsilon_pred, epsilon)\n",
    "    bar.set_description(\"Loss: {:.4f}\".format(loss))\n",
    "\n",
    "    opt.zero_grad()        \n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        grad_norm = torch.linalg.norm(X0.img.grad)\n",
    "        if i > 0:\n",
    "            alpha = 0.5\n",
    "            norm_track = alpha*norm_track + (1-alpha)*grad_norm\n",
    "        else:\n",
    "            norm_track = grad_norm \n",
    "                  \n",
    "    opt.step()\n",
    "\n",
    "    zero_t = torch.zeros((1,), device=device)\n",
    "    logits = classifier(X0.encode(), zero_t)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    selected = log_probs[range(len(logits)), classes.view(-1)] \n",
    "    Loss_label = selected.sum()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    Loss_label.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(X0.parameters(), 0.1*norm_track)\n",
    "    opt.step()\n",
    "    \n",
    "\n",
    "    if (i+1) % 25 == 0 or i == 0:\n",
    "        with torch.no_grad():\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "            ax.imshow(0.5*(X0.encode()+1)[0].cpu().numpy().transpose([1,2,0]))\n",
    "            ax.set_title('Inferred Image')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16833bc7",
   "metadata": {},
   "source": [
    "## 3. ImageNet + AdaGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c34c4d-1563-47e1-aaf0-b33635d7e5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from unet import UNetModel, EncoderUNetModel\n",
    "from attr_classifier import FaceAttrModel\n",
    "from diffusion import GaussianDiffusion\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbce21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DDPM model\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "diff_net = UNetModel(image_size=256, in_channels=3, out_channels=6, num_classes=1000,\n",
    "                     model_channels=256, num_res_blocks=2, channel_mult=(1, 1, 2, 2, 4, 4),\n",
    "                     attention_resolutions=[32,16,8], num_head_channels=64, dropout=0.0, resblock_updown=True, use_scale_shift_norm=True).to(device)\n",
    "diff_net.load_state_dict(torch.load('/root/Desktop/diffusionclean/guided-diffusion/models/256x256_diffusion.pt'))\n",
    "print('Loaded Diffusion Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a79033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(\n",
    "    image_size,\n",
    "    classifier_use_fp16,\n",
    "    classifier_width,\n",
    "    classifier_depth,\n",
    "    classifier_attention_resolutions,\n",
    "    classifier_use_scale_shift_norm,\n",
    "    classifier_resblock_updown,\n",
    "    classifier_pool,\n",
    "):\n",
    "    if image_size == 512:\n",
    "        channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n",
    "    elif image_size == 256:\n",
    "        channel_mult = (1, 1, 2, 2, 4, 4)\n",
    "    elif image_size == 128:\n",
    "        channel_mult = (1, 1, 2, 3, 4)\n",
    "    elif image_size == 64:\n",
    "        channel_mult = (1, 2, 3, 4)\n",
    "    else:\n",
    "        raise ValueError(f\"unsupported image size: {image_size}\")\n",
    "\n",
    "    attention_ds = []\n",
    "    for res in classifier_attention_resolutions.split(\",\"):\n",
    "        attention_ds.append(image_size // int(res))\n",
    "\n",
    "    return EncoderUNetModel(\n",
    "        image_size=image_size,\n",
    "        in_channels=3,\n",
    "        model_channels=classifier_width,\n",
    "        out_channels=1000,\n",
    "        num_res_blocks=classifier_depth,\n",
    "        attention_resolutions=tuple(attention_ds),\n",
    "        channel_mult=channel_mult,\n",
    "        use_fp16=classifier_use_fp16,\n",
    "        num_head_channels=64,\n",
    "        use_scale_shift_norm=classifier_use_scale_shift_norm,\n",
    "        resblock_updown=classifier_resblock_updown,\n",
    "        pool=classifier_pool,\n",
    "    )\n",
    "\n",
    "classifier = create_classifier(\n",
    "    image_size=256, \n",
    "    classifier_use_fp16=False, classifier_width=128, classifier_depth=2,\n",
    "    classifier_attention_resolutions=\"32,16,8\",  # 16\n",
    "    classifier_use_scale_shift_norm=True,  # False\n",
    "    classifier_resblock_updown=True,  # False\n",
    "    classifier_pool=\"attention\"\n",
    ").to(device)\n",
    "\n",
    "classifier.load_state_dict(torch.load(\"/root/Desktop/diffusionclean/guided-diffusion/models/256x256_clean_classifier.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a50ef-99c7-4515-bf8a-b53c42e7d9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InferenceModel, self).__init__()\n",
    "        # Inferred image\n",
    "        self.img = nn.Parameter(torch.randn(1,3,256,256))\n",
    "        self.img.requires_grad = True\n",
    "\n",
    "    def encode(self):\n",
    "        return self.img\n",
    "\n",
    "X0 = InferenceModel().to(device)\n",
    "# Inference procedure steps\n",
    "steps = 250   \n",
    "\n",
    "opt = torch.optim.Adamax(X0.parameters(), lr=1)\n",
    "\n",
    "diffusion = GaussianDiffusion(T=1000, schedule='linear')\n",
    "diff_net.eval()\n",
    "\n",
    "classes = torch.randint(low=0, high=1, size=(1,), device=device)\n",
    "\n",
    "bar = tqdm.tqdm(range(steps))\n",
    "for i, _ in enumerate(bar):\n",
    "    # Select t      \n",
    "    t = ((steps-i)/1.5 + (steps-i)/3*math.cos(i/10))/steps*800 + 200 # Linearly decreasing + cosine\n",
    "    t = np.array([t + np.random.randint(-50, 51) for _ in range(1)]).astype(int) # Add noise to t\n",
    "    t = np.clip(t, 1, diffusion.T)\n",
    "       \n",
    "    # Denoise\n",
    "    sample_img = X0.encode()\n",
    "    xt, epsilon = diffusion.sample(sample_img, t)       \n",
    "    t = torch.from_numpy(t).float().view(1)    \n",
    "    pred = diff_net(xt.float(), t.to(device), y=classes.to(device))   \n",
    "    epsilon_pred = pred[:,:3,:,:] # Use predicted noise only\n",
    "    \n",
    "    loss = F.mse_loss(epsilon_pred, epsilon)\n",
    "    bar.set_description(\"Loss: {:.4f}\".format(loss))\n",
    "\n",
    "    opt.zero_grad()        \n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        grad_norm = torch.linalg.norm(X0.img.grad)\n",
    "        if i > 0:\n",
    "            alpha = 0.5\n",
    "            norm_track = alpha*norm_track + (1-alpha)*grad_norm\n",
    "        else:\n",
    "            norm_track = grad_norm \n",
    "                  \n",
    "    opt.step()\n",
    "\n",
    "    zero_t = torch.zeros((1,), device=device)\n",
    "    logits = classifier(X0.encode(), zero_t)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    selected = log_probs[range(len(logits)), classes.view(-1)] \n",
    "    Loss_label = selected.sum()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    Loss_label.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(X0.parameters(), 0.1*norm_track)\n",
    "    opt.step()\n",
    "    \n",
    "\n",
    "    if (i+1) % 25 == 0 or i == 0:\n",
    "        with torch.no_grad():\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "            ax.imshow(0.5*(X0.encode()+1)[0].cpu().numpy().transpose([1,2,0]))\n",
    "            ax.set_title('Inferred Image')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abbe11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59054e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data = np.load('/root/Desktop/diffusion_priors/imagenet/temp/samples_4x256x256x3.npz')\n",
    "plt.imshow(data['arr_0'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a519959",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python clean_classifier_sample.py \\\n",
    "    --image_size 256 --batch_size 4 --num_samples 4 --save_base ./temp \\\n",
    "    --classifier_path models/256x256_classifier.pt \\\n",
    "    --model_path models/256x256_diffusion_uncond.pt \\\n",
    "    --class_cond False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "388f51ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000:   2%|▍                            | 4/250 [00:07<06:57,  1.70s/it]^C\n",
      "Loss: 0.0000:   2%|▍                            | 4/250 [00:08<09:08,  2.23s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"clean_classifier_sample.py\", line 179, in <module>\n",
      "    torch.nn.utils.clip_grad_norm_(X0.parameters(), 0.1*norm_track)\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\", line 33, in clip_grad_norm_\n",
      "    max_norm = float(max_norm)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python clean_classifier_sample.py \\\n",
    "    --image_size 256 --batch_size 4 --num_samples 4 --save_base ./temp \\\n",
    "    --classifier_path models/256x256_classifier.pt \\\n",
    "    --model_path models/256x256_diffusion.pt \\\n",
    "    --class_cond True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90a8550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008:   4%|█▏                          | 11/250 [00:18<05:58,  1.50s/it]^C\n",
      "Loss: 0.0008:   5%|█▎                          | 12/250 [00:19<06:24,  1.61s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"clean_classifier_sample.py\", line 150, in <module>\n",
      "    pred = diff_net(xt.float(), t, y=classes) \n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/Desktop/diffusion_priors/imagenet/unet.py\", line 661, in forward\n",
      "    h = module(h, emb)\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/Desktop/diffusion_priors/imagenet/unet.py\", line 75, in forward\n",
      "    x = layer(x, emb)\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/Desktop/diffusion_priors/imagenet/unet.py\", line 232, in forward\n",
      "    return checkpoint(\n",
      "  File \"/root/Desktop/diffusion_priors/imagenet/nn.py\", line 139, in checkpoint\n",
      "    return func(*inputs)\n",
      "  File \"/root/Desktop/diffusion_priors/imagenet/unet.py\", line 252, in _forward\n",
      "    h = out_rest(h)\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 139, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 457, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 453, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3 python clean_classifier_sample.py \\\n",
    "    --image_size 128 --batch_size 16 --num_samples 16 --save_base ./temp \\\n",
    "    --classifier_path models/128x128_classifier.pt \\\n",
    "    --model_path models/128x128_diffusion.pt \\\n",
    "    --class_cond True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7171717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0014:   2%|▋                            | 6/250 [00:14<09:18,  2.29s/it]^C\n",
      "Loss: 0.0014:   2%|▋                            | 6/250 [00:15<10:19,  2.54s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"clean_classifier_sample.py\", line 156, in <module>\n",
      "    bar.set_description(\"Loss: {:.4f}\".format(loss))\n",
      "  File \"/usr/local/conda/envs/ddpm/lib/python3.8/site-packages/torch/_tensor.py\", line 659, in __format__\n",
      "    return self.item().__format__(format_spec)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python clean_classifier_sample.py \\\n",
    "    --image_size 64 --batch_size 48 --num_samples 48 --save_base ./temp \\\n",
    "    --classifier_path models/64x64_classifier.pt \\\n",
    "    --model_path models/64x64_diffusion.pt \\\n",
    "    --class_cond True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b5f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ddpm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "36e62cac5a60872bc06aff40a4aa8139435b1d598e7fae1006b0e44b7fdda497"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
